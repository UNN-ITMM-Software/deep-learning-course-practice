{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим всё необходимое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import time\n",
    "import idx2numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим данные тренировочного набора MNIST для работы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры тренировочных данных для x и y: (60000, 784) и (60000, 10) соответственно\n",
      "Размеры тестовых данных для x и y: (10000, 784) и (10000, 784) соответственно\n"
     ]
    }
   ],
   "source": [
    "class_count = 10\n",
    "\n",
    "x_train = idx2numpy.convert_from_file('train-images.idx3-ubyte')\n",
    "y_train = idx2numpy.convert_from_file('train-labels.idx1-ubyte')\n",
    "x_test = idx2numpy.convert_from_file('t10k-images.idx3-ubyte')\n",
    "y_test = idx2numpy.convert_from_file('t10k-labels.idx1-ubyte')\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0], 28 * 28))\n",
    "x_test = x_test.reshape((x_test.shape[0], 28 * 28))\n",
    "\n",
    "y_train = np.eye(class_count)[y_train]\n",
    "y_test = np.eye(class_count)[y_test]\n",
    "\n",
    "print(\"Размеры тренировочных данных для x и y:\", x_train.shape, \"и\", y_train.shape, \"соответственно\")\n",
    "print(\"Размеры тестовых данных для x и y:\", x_test.shape, \"и\", x_test.shape, \"соответственно\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим класс NeuralNetwork, в котором будет реализован метод обратного распространения ошибки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, input_layer, hidden_layer, output_layer):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Вход\n",
    "        self.input_layer = input_layer\n",
    "        # Скрытый слой\n",
    "        self.hidden_layer = hidden_layer\n",
    "        # Выход\n",
    "        self.output_layer = output_layer\n",
    "\n",
    "        # Синаптические веса (входные сигналы от других нейронов)\n",
    "        self.w = [np.random.normal(0, np.sqrt(2 / (input_layer)), (input_layer, hidden_layer)),\n",
    "                  np.random.normal(0, np.sqrt(6 / (hidden_layer + output_layer)), (hidden_layer, output_layer))]\n",
    "\n",
    "        # Воздействие внешней среды (сдвиги), представляется свободным членом\n",
    "        self.b = [np.zeros(hidden_layer),\n",
    "                  np.zeros(output_layer)]\n",
    "\n",
    "    # Функция активации Relu (положительная срезка) для внутренного слоя\n",
    "    def relu(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    # Производная функции Relu\n",
    "    def derived_relu(self, x):\n",
    "        return np.where(x > 0.0, 1, 0)\n",
    "\n",
    "    # Функция активации SoftMax для внешного слоя\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x)\n",
    "        return np.divide(exp_x.T, (np.sum(exp_x, axis = 1))).T\n",
    "\n",
    "    # Точность\n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n",
    "\n",
    "    # Функция ошибки - кросс-энтропия для задачи классификации\n",
    "    def compute_cross_entropy_loss(self, y_true, y_pred):\n",
    "        return np.mean(-np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "    # Прямой проход\n",
    "    def forward(self, x):\n",
    "        # Сумма синапсов с некоторыми весами и внешнего воздействия\n",
    "        x = np.matmul(x, self.w[0]) + self.b[0]\n",
    "        # Запоминаем значения, полученные на внутреннем слое\n",
    "        self.inner_layer_w = x.copy()\n",
    "\n",
    "        # Преобразования сигнала с помощью функции активации (цель - ограничить амплитуду выходного сигнала)\n",
    "        x = self.relu(x)\n",
    "        # Запоминаем веса, полученные после активации\n",
    "        self.h = x.copy()\n",
    "\n",
    "        # То же самое, но от скрытого слоя к выходному\n",
    "        x = np.matmul(x, self.w[1]) + self.b[1]\n",
    "        # Функции активации\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "    # Обратный проход\n",
    "    # На базе минипакетного стохастического градиентного спуска\n",
    "    def backward(self, x_train, y_pred, y_true): # Обратный проход\n",
    "        \n",
    "        # Делим единицу на число примеров в пакете (не забываем про минус)\n",
    "        one_devide_l = - 1 / x_train.shape[0]\n",
    "        \n",
    "        d_z2 = y_pred - y_true\n",
    "\n",
    "        # Частная производная по весам w2 связей от скрытого слоя к выходному\n",
    "        # Производная логарифма сокращается с производной Softmax, которая равна сама себе\n",
    "        # Из производной сложной функции по итогу остаётся только Relu\n",
    "        d_w2 = one_devide_l * np.matmul(self.h.T, d_z2)\n",
    "\n",
    "        # Частная производная по весам w1 связей от входного слоя к скрытому\n",
    "        # Производная логарифма сокращается с производной Softmax, которая равна сама себе\n",
    "        # Веса w2 выносятся как константа перед производной\n",
    "        # Производная от сложной Relu даёт производной себя, помноженную на x\n",
    "        d_z1 = np.matmul(d_z2, self.w[1].T) * self.derived_relu(self.inner_layer_w)\n",
    "        d_w1 = one_devide_l * np.matmul(x_train.T, d_z1)\n",
    "\n",
    "        # Частная производная по сдвигам b2 и b1.\n",
    "        # Почти повторяет dw2 и dw1, но производные сложных функций дают просто Softmax * 1 и Relu' * 1\n",
    "        d_b2 = one_devide_l * np.sum(d_z2, axis=0)\n",
    "        d_b1 = one_devide_l * np.sum(d_z1, axis=0)\n",
    "\n",
    "        # Корректировка синаптических весов\n",
    "        self.w[1] = self.w[1] + self.learning_rate * d_w2\n",
    "        self.w[0] = self.w[0] + self.learning_rate * d_w1\n",
    "        self.b[1] = self.b[1] + self.learning_rate * d_b2\n",
    "        self.b[0] = self.b[0] + self.learning_rate * d_b1\n",
    "\n",
    "    def train(self, x_train, y_train, x_test, y_test, epochs, learning_rate, batch_size):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        start_of_all_train_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            start_of_train_time = time.time()\n",
    "\n",
    "            step_count = x_train.shape[0] // batch_size\n",
    "            if x_train.shape[0] % batch_size > 0:\n",
    "                step_count += 1\n",
    "\n",
    "            for batch_id in range(step_count):\n",
    "                start_id = batch_id * batch_size\n",
    "                finish_id = min((batch_id + 1) * batch_size, y_train.shape[0])\n",
    "\n",
    "                y_pred = self.forward(x_train[start_id:finish_id])\n",
    "                self.backward(x_train[start_id:finish_id], y_pred, y_train[start_id:finish_id])\n",
    "\n",
    "            end_of_train_time = time.time()\n",
    "            epoch_train_time = end_of_train_time - start_of_train_time\n",
    "\n",
    "            y_pred = self.forward(x_train)\n",
    "            error_train = self.compute_cross_entropy_loss(y_train, y_pred)\n",
    "            acc_train = self.compute_accuracy(y_train, y_pred)\n",
    "            \n",
    "            print(\"Эпоха №\", (epoch + 1), end='')\n",
    "            print(f\"\\tВремя обучение: {epoch_train_time:.2f} сек\", end='')\n",
    "            print(f\"\\tОшибка: {error_train:.2f}\", end='')\n",
    "            print(f\"\\tТочность: {acc_train:.2f}\")\n",
    "\n",
    "        end_of_all_train_time = time.time()\n",
    "        all_train_time = end_of_all_train_time - start_of_all_train_time\n",
    "        print(f\"Общее время обучение: {all_train_time:.2f} сек\\n\")\n",
    "            \n",
    "        y_pred = self.forward(x_test)\n",
    "        error_test = self.compute_cross_entropy_loss(y_test, y_pred)\n",
    "        acc_test = self.compute_accuracy(y_test, y_pred)\n",
    "\n",
    "        print(f\"Ошибка на тестовой выборке: {error_test:.2f}\")\n",
    "        print(f\"Точность на тестовой выборке: {acc_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха № 1\tВремя обучение: 4.18 сек\tОшибка: 0.20\tТочность: 0.94\n",
      "Эпоха № 2\tВремя обучение: 3.75 сек\tОшибка: 0.14\tТочность: 0.96\n",
      "Эпоха № 3\tВремя обучение: 3.56 сек\tОшибка: 0.11\tТочность: 0.97\n",
      "Эпоха № 4\tВремя обучение: 3.58 сек\tОшибка: 0.09\tТочность: 0.98\n",
      "Эпоха № 5\tВремя обучение: 3.54 сек\tОшибка: 0.07\tТочность: 0.98\n",
      "Эпоха № 6\tВремя обучение: 4.07 сек\tОшибка: 0.06\tТочность: 0.98\n",
      "Эпоха № 7\tВремя обучение: 3.97 сек\tОшибка: 0.06\tТочность: 0.99\n",
      "Эпоха № 8\tВремя обучение: 4.45 сек\tОшибка: 0.05\tТочность: 0.99\n",
      "Эпоха № 9\tВремя обучение: 3.55 сек\tОшибка: 0.04\tТочность: 0.99\n",
      "Эпоха № 10\tВремя обучение: 3.54 сек\tОшибка: 0.04\tТочность: 0.99\n",
      "Эпоха № 11\tВремя обучение: 3.54 сек\tОшибка: 0.04\tТочность: 0.99\n",
      "Эпоха № 12\tВремя обучение: 3.68 сек\tОшибка: 0.03\tТочность: 0.99\n",
      "Эпоха № 13\tВремя обучение: 3.50 сек\tОшибка: 0.03\tТочность: 0.99\n",
      "Эпоха № 14\tВремя обучение: 3.36 сек\tОшибка: 0.03\tТочность: 0.99\n",
      "Эпоха № 15\tВремя обучение: 3.46 сек\tОшибка: 0.03\tТочность: 0.99\n",
      "Эпоха № 16\tВремя обучение: 3.35 сек\tОшибка: 0.02\tТочность: 0.99\n",
      "Эпоха № 17\tВремя обучение: 3.38 сек\tОшибка: 0.02\tТочность: 0.99\n",
      "Эпоха № 18\tВремя обучение: 3.38 сек\tОшибка: 0.02\tТочность: 0.99\n",
      "Эпоха № 19\tВремя обучение: 3.39 сек\tОшибка: 0.02\tТочность: 1.00\n",
      "Эпоха № 20\tВремя обучение: 3.42 сек\tОшибка: 0.02\tТочность: 1.00\n",
      "Общее время обучение: 84.21 сек\n",
      "\n",
      "Ошибка на тестовой выборке: 0.07\n",
      "Точность на тестовой выборке: 0.98\n"
     ]
    }
   ],
   "source": [
    "output_layer = class_count\n",
    "\n",
    "batch_size = 64\n",
    "hidden_layer = 300\n",
    "lr = 0.1\n",
    "epoch_count = 20\n",
    "\n",
    "nn = NeuralNetwork(x_train.shape[1], hidden_layer, output_layer)\n",
    "nn.train(x_train, y_train, x_test, y_test, epoch_count, lr, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "931bee2713cf3abade0edc19bfae1c4bb717028b3af4783f8e50679a9fd6b43e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

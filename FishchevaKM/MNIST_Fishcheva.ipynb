{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "class NeuralNetwork(object):\n",
        "\n",
        "    def __init__(self, inputs, hidden, outputs, activation='relu', output_act='softmax'):\n",
        "\n",
        "        # Hidden layer activation function\n",
        "        self.activation = relu\n",
        "        self.activation_prime = relu\n",
        "\n",
        "        # Output layer activation function\n",
        "        self.output_act = softmax\n",
        "        self.output_act_prime = softmax_prime\n",
        "\n",
        "        # Weights initializarion\n",
        "        self.wi = np.random.randn(inputs, hidden) / np.sqrt(inputs)\n",
        "        self.wo = np.random.randn(hidden + 1, outputs) / np.sqrt(hidden)\n",
        "\n",
        "        # Weights updates initialization\n",
        "        self.updatei = 0\n",
        "        self.updateo = 0\n",
        "\n",
        "    def feedforward(self, X):\n",
        "\n",
        "        # Hidden layer activation\n",
        "        ah = self.activation(np.dot(X, self.wi))\n",
        "\n",
        "        # Adding bias to the hidden layer results\n",
        "        ah = np.concatenate((np.ones(1).T, np.array(ah)))\n",
        "\n",
        "        # Outputs\n",
        "        y = self.output_act(np.dot(ah, self.wo))\n",
        "\n",
        "        # Return the results\n",
        "        return y\n",
        "\n",
        "    def fit(self, X, y, epochs=10, learning_rate=0.2, learning_rate_decay=0, momentum=0, verbose=0):\n",
        "\n",
        "        # Timer start\n",
        "        startTime = time.time()\n",
        "\n",
        "        # Epochs loop\n",
        "        for k in range(epochs):\n",
        "\n",
        "            # Dataset loop\n",
        "            for i in range(X.shape[0]):\n",
        "                # Hidden layer activation\n",
        "                ah = self.activation(np.dot(X[i], self.wi))\n",
        "\n",
        "                # Adding bias to the hidden layer\n",
        "                ah = np.concatenate((np.ones(1).T, np.array(ah)))\n",
        "\n",
        "                # Output activation\n",
        "                ao = self.output_act(np.dot(ah, self.wo))\n",
        "\n",
        "                # Deltas\n",
        "                deltao = np.multiply(self.output_act_prime(ao), y[i] - ao)\n",
        "                deltai = np.multiply(self.activation_prime(ah), np.dot(self.wo, deltao))\n",
        "\n",
        "                # Weights update with momentum\n",
        "                self.updateo = momentum * self.updateo + np.multiply(learning_rate, np.outer(ah, deltao))\n",
        "                self.updatei = momentum * self.updatei + np.multiply(learning_rate, np.outer(X[i], deltai[1:]))\n",
        "\n",
        "                # Weights update\n",
        "                self.wo += self.updateo\n",
        "                self.wi += self.updatei\n",
        "\n",
        "            # Print training status\n",
        "            if verbose == 1:\n",
        "                print('EPOCH: {0:4d}/{1:4d}\\t\\tLearning rate: {2:4f}\\t\\tElapse time [seconds]: {3:5f}'.format(k, epochs,\n",
        "                                                                                                              learning_rate,\n",
        "                                                                                                              time.time() - startTime))\n",
        "\n",
        "            # Learning rate update\n",
        "            learning_rate = learning_rate * (1 - learning_rate_decay)\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        # Allocate memory for the outputs\n",
        "        y = np.zeros([X.shape[0], self.wo.shape[1]])\n",
        "\n",
        "        # Loop the inputs\n",
        "        for i in range(0, X.shape[0]):\n",
        "            y[i] = self.feedforward(X[i])\n",
        "\n",
        "        # Return the results\n",
        "        return y\n",
        "\n",
        "\n",
        "# Activation functions\n",
        "def softmax(x):\n",
        "    return (np.exp(np.array(x)) / np.sum(np.exp(np.array(x))))\n",
        "\n",
        "\n",
        "def softmax_prime(x):\n",
        "    return softmax(x) * (1.0 - softmax(x))\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(x, 0)"
      ],
      "metadata": {
        "id": "IbujnEC1zpFb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEdglASGzk_W",
        "outputId": "09134421-e742-418d-c538-e7e76b82b978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:    0/  20\t\tLearning rate: 0.100000\t\tElapse time [seconds]: 1.457399\n",
            "EPOCH:    1/  20\t\tLearning rate: 0.099000\t\tElapse time [seconds]: 2.750824\n",
            "EPOCH:    2/  20\t\tLearning rate: 0.098010\t\tElapse time [seconds]: 3.577000\n",
            "EPOCH:    3/  20\t\tLearning rate: 0.097030\t\tElapse time [seconds]: 3.993360\n",
            "EPOCH:    4/  20\t\tLearning rate: 0.096060\t\tElapse time [seconds]: 4.434690\n",
            "EPOCH:    5/  20\t\tLearning rate: 0.095099\t\tElapse time [seconds]: 4.857426\n",
            "EPOCH:    6/  20\t\tLearning rate: 0.094148\t\tElapse time [seconds]: 5.294990\n",
            "EPOCH:    7/  20\t\tLearning rate: 0.093207\t\tElapse time [seconds]: 5.737790\n",
            "EPOCH:    8/  20\t\tLearning rate: 0.092274\t\tElapse time [seconds]: 6.157300\n",
            "EPOCH:    9/  20\t\tLearning rate: 0.091352\t\tElapse time [seconds]: 6.631336\n",
            "EPOCH:   10/  20\t\tLearning rate: 0.090438\t\tElapse time [seconds]: 7.065027\n",
            "EPOCH:   11/  20\t\tLearning rate: 0.089534\t\tElapse time [seconds]: 7.558760\n",
            "EPOCH:   12/  20\t\tLearning rate: 0.088638\t\tElapse time [seconds]: 7.979780\n",
            "EPOCH:   13/  20\t\tLearning rate: 0.087752\t\tElapse time [seconds]: 8.445494\n",
            "EPOCH:   14/  20\t\tLearning rate: 0.086875\t\tElapse time [seconds]: 8.868617\n",
            "EPOCH:   15/  20\t\tLearning rate: 0.086006\t\tElapse time [seconds]: 9.352313\n",
            "EPOCH:   16/  20\t\tLearning rate: 0.085146\t\tElapse time [seconds]: 9.806256\n",
            "EPOCH:   17/  20\t\tLearning rate: 0.084294\t\tElapse time [seconds]: 10.284347\n",
            "EPOCH:   18/  20\t\tLearning rate: 0.083451\t\tElapse time [seconds]: 10.731519\n",
            "EPOCH:   19/  20\t\tLearning rate: 0.082617\t\tElapse time [seconds]: 11.206544\n",
            "\n",
            "Classification report for classifier:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        27\n",
            "           1       0.94      0.97      0.96        35\n",
            "           2       1.00      0.97      0.99        36\n",
            "           3       0.91      1.00      0.95        29\n",
            "           4       0.97      0.97      0.97        30\n",
            "           5       0.97      0.97      0.97        40\n",
            "           6       1.00      0.98      0.99        44\n",
            "           7       0.95      1.00      0.97        39\n",
            "           8       1.00      0.92      0.96        39\n",
            "           9       0.97      0.95      0.96        41\n",
            "\n",
            "    accuracy                           0.97       360\n",
            "   macro avg       0.97      0.97      0.97       360\n",
            "weighted avg       0.97      0.97      0.97       360\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "def targetToVector(x):\n",
        "    # Vector\n",
        "    a = np.zeros([len(x), 10])\n",
        "    for i in range(0, len(x)):\n",
        "        a[i, x[i]] = 1\n",
        "    return a\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Digits dataset loading\n",
        "    digits = datasets.load_digits()\n",
        "    X = preprocessing.scale(digits.data.astype(float))\n",
        "    y = targetToVector(digits.target)\n",
        "\n",
        "    # Cross valitation\n",
        "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "    # Neural Network initialization\n",
        "    NN = NeuralNetwork(64, 300, 10, activation='relu', output_act='softmax')\n",
        "    NN.fit(X_train, y_train, epochs=20, learning_rate=.1, learning_rate_decay=.01, verbose=1)\n",
        "\n",
        "    # NN predictions\n",
        "    y_predicted = NN.predict(X_test)\n",
        "\n",
        "    # Metrics\n",
        "    y_predicted = np.argmax(y_predicted, axis=1).astype(int)\n",
        "    y_test = np.argmax(y_test, axis=1).astype(int)\n",
        "\n",
        "    print(\"\\nClassification report for classifier:\\n\\n%s\\n\"\n",
        "          % (metrics.classification_report(y_test, y_predicted)))\n",
        "    #print(\"Confusion matrix:\\n\\n%s\" % metrics.confusion_matrix(y_test, y_predicted))\n"
      ]
    }
  ]
}